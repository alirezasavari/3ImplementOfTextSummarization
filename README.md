
implementation 1: Key Phrase Extraction and Summarization with spaCy and PyTextRank**

This code uses spaCy and the PyTextRank library to extract key phrases and generate a summary from a given text. It begins by loading a pre-trained spaCy model, `en_core_web_lg`, which provides large English vocabulary and word vectors. Then, it adds TextRank (from PyTextRank) as a pipeline component. TextRank is a graph-based algorithm for ranking words and phrases in the text, which helps identify the most significant phrases and sentences. After loading a sample text, the code processes it through the pipeline, generating a summary of top-ranked sentences by limiting phrases and sentences. This summary highlights the most relevant parts of the text. Additionally, the code extracts the top-ranked phrases, displaying each phrase with a rank score that reflects its contextual importance. This approach can be used for tasks like document summarization and extracting key information from large texts.

---

implementation 2: Summarization with Pegasus using Direct Model Calls and Transformers Pipeline**

This code leverages the Pegasus model (specifically, the `google/pegasus-xsum` variant) from the Hugging Face Transformers library to generate text summaries. Pegasus is a transformer-based model pre-trained specifically for abstractive summarization tasks. The code initializes a tokenizer and model for Pegasus, then provides an extensive input text about Napoleon Bonaparteâ€™s historical impact. First, it tokenizes the text, preparing it for model input with truncation, padding, and tensor formatting for PyTorch compatibility. Using specified parameters like `max_length`, `min_length`, and `num_beams`, the model generates a summary by directly encoding the tokenized input into a concise text output. The code then initializes a Hugging Face pipeline for summarization, allowing the same Pegasus model to generate a summary more conveniently. The pipeline simplifies the process by automatically handling tokenization and text generation. The summarizer also takes parameters for length control, enabling concise yet informative summaries. This code offers flexibility between direct model use and a streamlined pipeline for summarization.

---

implementation 3: Summarization with Pegasus using Custom Functions**

This code builds on the Pegasus summarization capabilities, defining two functions to generate summaries. The first function, `generate_summary_from_model`, uses a pre-trained Pegasus model directly, allowing custom control over parameters like `max_length`, `min_length`, `length_penalty`, and `num_beams`. This function provides a customizable summarization process for users who require specific output characteristics. The second function, `generate_summary_with_pipeline`, utilizes the Hugging Face Transformers pipeline. This method abstracts the underlying complexity, making it easier for users to generate summaries by simply adjusting parameters like `min_length` and `max_length`. Both functions load the same Pegasus model and tokenizer, but the pipeline function enables a more accessible, straightforward approach to summarization. Sample text about Napoleon Bonaparte demonstrates both methods, generating two summaries to show the output differences between direct model calls and the pipeline. This code effectively demonstrates how to wrap summarization functions for flexible, reusable summarization applications.
